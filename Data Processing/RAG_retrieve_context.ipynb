{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OPERATIONALIZE FRAMEWORK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7117859e596637"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d1acee7295b132"
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "from files_to_database import main_to_database\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import chromadb\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.storage import create_kv_docstore, LocalFileStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_core.prompts import ChatPromptTemplate, format_document, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from getpass import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T14:02:17.570575Z",
     "start_time": "2024-04-26T14:02:17.565944Z"
    }
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Embedding models (local)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d10fa1d26c27915"
  },
  {
   "cell_type": "code",
   "source": [
    "# # For Apple Silicon users: run the following code to make use of MPS (Apple's Metal Performance Shaders) for faster computation\n",
    "# import torch\n",
    "# \n",
    "# # set device to MPS\n",
    "# device = torch.device(\"mps\")\n",
    "# \n",
    "# # empty cache and set memory fraction\n",
    "# torch.mps.empty_cache()\n",
    "# torch.mps.set_per_process_memory_fraction(0.9)\n",
    "# \n",
    "# # choose embeddings model\n",
    "# multilingual = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# \n",
    "# # local embedding model, download to cache folder\n",
    "# embedding_model = SentenceTransformer(multilingual, cache_folder=\"../Data/sentence_transformers\", device=device)\n",
    "# \n",
    "# embeddings_retrieve = HuggingFaceEmbeddings(model_name=multilingual, cache_folder=\"../Data/sentence_transformers\")\n",
    "# \n",
    "# # move model to MPS\n",
    "# embedding_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:01.651635Z",
     "start_time": "2024-04-26T13:36:01.648955Z"
    }
   },
   "id": "7f4936fbc15e7142",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# local embedding model, download to cache folder\n",
    "multilingual = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# model used for document embedding\n",
    "embedding_model = SentenceTransformer(\n",
    "    model_name_or_path=multilingual, \n",
    "    cache_folder=\"../Data/sentence_transformers\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:04.469527Z",
     "start_time": "2024-04-26T13:36:01.652556Z"
    }
   },
   "id": "15e530d33307636f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# model used for query embedding\n",
    "embeddings_retrieve = HuggingFaceEmbeddings(\n",
    "    model_name=multilingual,\n",
    "    cache_folder=\"../Data/sentence_transformers\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.638406Z",
     "start_time": "2024-04-26T13:36:04.470603Z"
    }
   },
   "id": "e92e96ae0a8f51dd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Chroma client setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6747175bcf9e8572"
  },
  {
   "cell_type": "code",
   "source": [
    "# initiate the chroma client, which is the interface to the database\n",
    "database_path = \"../Data/my_vectordb\"\n",
    "chroma_client = chromadb.PersistentClient(path=database_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.754311Z",
     "start_time": "2024-04-26T13:36:06.640429Z"
    }
   },
   "id": "9bfeaab0ffd4cd01",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.761125Z",
     "start_time": "2024-04-26T13:36:06.755259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print the collections\n",
    "chroma_client.list_collections()"
   ],
   "id": "b3c228a358d7148b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=rijksoverheid),\n",
       " Collection(name=ibestuur),\n",
       " Collection(name=tenderned),\n",
       " Collection(name=binnenlands_bestuur)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Retrievers for all databases"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "726761a1b9acd248"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1 Parent / child splitters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f736d27b4cd40bd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.764823Z",
     "start_time": "2024-04-26T13:36:06.762146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the retrievers, parent and child splitters, MAKE SURE TO CHANGE ALSO IN files_to_database.py\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                                 chunk_overlap=0)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=128,\n",
    "                                                chunk_overlap=0)"
   ],
   "id": "32d8a1eb43ff4f1d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.2 Rijksoverheid retriever"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2819dbf6eb37a78"
  },
  {
   "cell_type": "code",
   "source": [
    "# rijksoverheid database\n",
    "rijksoverheid_db= Chroma(\n",
    "    collection_name=\"rijksoverheid\",\n",
    "    client=chroma_client,\n",
    "    persist_directory=\"../Data/my_vectordb\",\n",
    "    embedding_function=embeddings_retrieve,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.773694Z",
     "start_time": "2024-04-26T13:36:06.765802Z"
    }
   },
   "id": "2104983993013888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "columns_to_embed = [\"content\"]\n",
    "columns_to_metadata = [\"id\", \"type\", \"title\", \"canonical\", \"introduction\", \"lastmodified\", \"available\", \"initialdate\"]\n",
    "\n",
    "# rijksoverheid retriever\n",
    "full_path = os.path.abspath(\"../Data/my_vectordb/full_documents/rijksoverheid\")\n",
    "\n",
    "fs = LocalFileStore(full_path)\n",
    "store = create_kv_docstore(fs)\n",
    "\n",
    "rijksoverheid_db_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=rijksoverheid_db,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    child_metadata_field=columns_to_metadata,\n",
    "    parent_splitter=parent_splitter,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.777710Z",
     "start_time": "2024-04-26T13:36:06.774625Z"
    }
   },
   "id": "f5e1ac2595323171",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.3 ibestuur retriever"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd324eb76a441ca7"
  },
  {
   "cell_type": "code",
   "source": [
    "# ibestuur database\n",
    "ibestuur_db= Chroma(\n",
    "    collection_name=\"ibestuur\",\n",
    "    client=chroma_client,\n",
    "    persist_directory=\"../Data/my_vectordb\",\n",
    "    embedding_function=embeddings_retrieve,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.782339Z",
     "start_time": "2024-04-26T13:36:06.778811Z"
    }
   },
   "id": "e18ba53e2156c29c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# ibestuur retriever\n",
    "ibestuur_retriever = ibestuur_db.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.785550Z",
     "start_time": "2024-04-26T13:36:06.783219Z"
    }
   },
   "id": "68ff0e95783bc424",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.4 binnenlandsbestuur retriever"
   ],
   "id": "cd2d469765163dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.790624Z",
     "start_time": "2024-04-26T13:36:06.786304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# binnenlandsbestuur database\n",
    "binnenlandsbestuur_db= Chroma(\n",
    "    collection_name=\"binnenlands_bestuur\",\n",
    "    client=chroma_client,\n",
    "    persist_directory=\"../Data/my_vectordb\",\n",
    "    embedding_function=embeddings_retrieve,\n",
    ")"
   ],
   "id": "fc11fff105820003",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.793528Z",
     "start_time": "2024-04-26T13:36:06.791411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# binnenlandsbestuur retriever\n",
    "binnenlandsbestuur_retriever = binnenlandsbestuur_db.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")"
   ],
   "id": "9e4d30683e35e72b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.5 tenderned retriever"
   ],
   "id": "e0f0982b9b38839e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.797896Z",
     "start_time": "2024-04-26T13:36:06.796026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tenderned database"
   ],
   "id": "5189cc550b35c0f5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. LLM from Inference Endpoints API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27d163c5d582fc64"
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure to replace these values with your personal API URL and KEY\n",
    "# API_URL = \"https://oi6h8u843v8nt5qt.eu-west-1.aws.endpoints.huggingface.cloud\"\n",
    "# API_KEY = getpass(\"Enter your API KEY:\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.800476Z",
     "start_time": "2024-04-26T13:36:06.798560Z"
    }
   },
   "id": "d361d0c15bc9ccbe",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# LLM model (Hugging Face)\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     endpoint_url=API_URL,\n",
    "#     huggingfacehub_api_token=API_KEY,\n",
    "#     temperature=0.1,\n",
    "#     max_new_tokens=2048,\n",
    "#     model_kwargs={\"max_input_length\": 2048, \"max_length\": 2048, \"max_num_tokens\": 2048}\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:06.803729Z",
     "start_time": "2024-04-26T13:36:06.801263Z"
    }
   },
   "id": "9681fc664d89696d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.156947Z",
     "start_time": "2024-04-26T13:36:06.804660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HF_token = getpass(\"Enter your Hugging Face API Token:\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_token\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=HF_token,\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    model_kwargs={\"temperature\":0.5, \"max_new_tokens\":512, \"max_length\":64}\n",
    ")\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"gpt2\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 128}\n",
    "# )"
   ],
   "id": "484e85f05be86d9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Load Framework"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6798d4e7f32f52ba"
  },
  {
   "cell_type": "code",
   "source": [
    "# prevent reading extra unnamed column\n",
    "framework = pd.read_csv(\"../Results/framework_questions_translated.csv\", usecols=[' #', '2022 GTMI Indicators & Sub-indicators NL', 'Response options & Data format NL'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.164462Z",
     "start_time": "2024-04-26T13:36:17.158048Z"
    }
   },
   "id": "c5a6acee52ef7282",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "framework = framework.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.167628Z",
     "start_time": "2024-04-26T13:36:17.165462Z"
    }
   },
   "id": "79abd316eef2f4c9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.175539Z",
     "start_time": "2024-04-26T13:36:17.168512Z"
    }
   },
   "cell_type": "code",
   "source": "framework",
   "id": "3dccb776546d7e8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       #           2022 GTMI Indicators & Sub-indicators NL  \\\n",
       "0    I-1  Is er een gedeeld cloud platform beschikbaar v...   \n",
       "1  I-1.1              Naam van het Overheids Cloud platform   \n",
       "2  I-1.2                     Cloud platform / strategie URL   \n",
       "3  I-1.3  Overheids Cloud gelanceerd / zal worden gelanc...   \n",
       "4  I-1.4                    Type beschikbaar cloud platform   \n",
       "\n",
       "                   Response options & Data format NL  \n",
       "0  0= Nee, 1= Alleen cloud strategie/beleid (nog ...  \n",
       "1                                              Tekst  \n",
       "2                                                URL  \n",
       "3                                               YYYY  \n",
       "4  0= Onbekend, 1= Publiek (Commercieel), 2= Priv...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>2022 GTMI Indicators &amp; Sub-indicators NL</th>\n",
       "      <th>Response options &amp; Data format NL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-1</td>\n",
       "      <td>Is er een gedeeld cloud platform beschikbaar v...</td>\n",
       "      <td>0= Nee, 1= Alleen cloud strategie/beleid (nog ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-1.1</td>\n",
       "      <td>Naam van het Overheids Cloud platform</td>\n",
       "      <td>Tekst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-1.2</td>\n",
       "      <td>Cloud platform / strategie URL</td>\n",
       "      <td>URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-1.3</td>\n",
       "      <td>Overheids Cloud gelanceerd / zal worden gelanc...</td>\n",
       "      <td>YYYY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-1.4</td>\n",
       "      <td>Type beschikbaar cloud platform</td>\n",
       "      <td>0= Onbekend, 1= Publiek (Commercieel), 2= Priv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Define functions to operationalize framework",
   "id": "9b441611b125b63a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.1 Generate prompt",
   "id": "35a099b27d10d3e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:59:54.489891Z",
     "start_time": "2024-04-26T13:59:54.485872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # chat prompt template\n",
    "template = \"\"\"\n",
    "Je bent 'GovTech-GPT', een geavanceerde AI-assistent met uitgebreide expertise in digitale technologieën specifiek gericht op toepassingen binnen de Nederlandse overheid. Je belangrijkste taak is het ondersteunen bij het operationaliseren van e-gov benchmarking frameworks. Je antwoordt altijd op basis van de meest recente gegevens en inzichten, en houdt rekening met de specifieke context van de Nederlandse overheid. Antwoorden geef je alleen volgens het gespecificeerde dataformat, waarbij je, indien mogelijk, het cijfer gebruikt en niet de tekst. Voeg verder geen enkele tekst, toelichting of uitleg meer toe. Als je het antwoord niet weet, geef je geen fictieve informatie of uitleg, maar antwoord enkel en alleen met: 'Geen antwoord.'  \\n\\n\"\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "DATA FORMAT: {data_format}\n",
    "\n",
    "VRAAG: {question}\n",
    "\n",
    "ANTWOORD: \n",
    "\"\"\""
   ],
   "id": "75018b7a065aa841",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:59:55.095594Z",
     "start_time": "2024-04-26T13:59:55.093655Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e45e7686d26e0d77",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.186400Z",
     "start_time": "2024-04-26T13:36:17.182387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# chat prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "id": "66e12707fa9efda8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.190023Z",
     "start_time": "2024-04-26T13:36:17.187461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# output parser\n",
    "output_parser = StrOutputParser()"
   ],
   "id": "9063bce2c16031d1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.2 Format context",
   "id": "504a0dde5ba608b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T13:36:17.194898Z",
     "start_time": "2024-04-26T13:36:17.191195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# format the context for input\n",
    "def format_context(context): \n",
    "    context_string = \"\"\n",
    "    \n",
    "    for i in range(len(context[\"context_ibestuur\"])):\n",
    "        context_string += f\"{dict(context['context_ibestuur'][i])['page_content']}\\n\"\n",
    "    for i in range(len(context[\"context_rijksoverheid\"])):\n",
    "        context_string += f\"{dict(context['context_rijksoverheid'][i])['page_content']}\\n\"\n",
    "\n",
    "    return context_string"
   ],
   "id": "6ec18982bbffc61d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.3 Retrieval setup",
   "id": "25a256cf2e926006"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.4 Chain setup",
   "id": "15ef9b8300d7b44e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### with context",
   "id": "e1fba36e8f990da1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:05:33.366462Z",
     "start_time": "2024-04-26T14:05:33.363517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = framework['2022 GTMI Indicators & Sub-indicators NL'].iloc[0]\n",
    "data_format1 = framework['Response options & Data format NL'].iloc[0]"
   ],
   "id": "76f2064156c157fa",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:08:13.742233Z",
     "start_time": "2024-04-26T14:08:13.739613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fill in the template\n",
    "class FormatDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return \"{\" + key + \"}\""
   ],
   "id": "b95c72d8b128fe8a",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:11:08.585907Z",
     "start_time": "2024-04-26T14:11:08.581821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def return_prompt(template, data_format):\n",
    "    formatter = string.Formatter()\n",
    "    mapping = FormatDict(data_format=data_format)\n",
    "    prompt_string = formatter.vformat(template, (), mapping)\n",
    "    return ChatPromptTemplate.from_template(prompt_string)\n",
    "    \n",
    "\n",
    "def format_docs(docs):\n",
    "    content = \"\\n\\n ------------\".join(doc.page_content for doc in docs)\n",
    "    urls = [doc.metadata[\"canonical\"] for doc in docs]\n",
    "    return content\n",
    "\n",
    "def retrieve_answer(output):\n",
    "    return output.content"
   ],
   "id": "706fd60158cc89c9",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "prompt = return_prompt(template, data_format1)",
   "id": "f8c3486ce23f9019"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:33:11.477007Z",
     "start_time": "2024-04-26T14:33:11.473742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Variation \n",
    "rag_chain = ( \n",
    "            RunnableParallel(context = rijksoverheid_db_retriever | format_docs, \n",
    "                             question = RunnablePassthrough() ) |\n",
    "            RunnableParallel(answer= prompt | llm | retrieve_answer,  question = itemgetter(\"question\"),  context = itemgetter(\"context\") ) \n",
    ")"
   ],
   "id": "dff5151ac3449d6a",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:33:13.421468Z",
     "start_time": "2024-04-26T14:33:11.938455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "rag_chain.invoke(\"When was SVM invented?\")"
   ],
   "id": "8113ce8496bf9735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 3:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 3:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 4:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 4:chain:RunnableSequence > 6:chain:format_docs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 4:chain:RunnableSequence > 6:chain:format_docs] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 4:chain:RunnableSequence] [17ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question>] [20ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"context\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\",\n",
      "  \"question\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"context\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\",\n",
      "  \"question\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 8:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"context\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\",\n",
      "  \"question\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 8:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 9:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"context\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\",\n",
      "  \"question\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 9:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 10:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"context\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\",\n",
      "  \"question\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 10:chain:RunnableSequence > 11:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"context\": \". ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\",\n",
      "  \"question\": \"When was SVM invented?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 10:chain:RunnableSequence > 11:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 10:chain:RunnableSequence > 12:llm:HuggingFaceHub] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nJe bent 'GovTech-GPT', een geavanceerde AI-assistent met uitgebreide expertise in digitale technologieën specifiek gericht op toepassingen binnen de Nederlandse overheid. Je belangrijkste taak is het ondersteunen bij het operationaliseren van e-gov benchmarking frameworks. Je antwoordt altijd op basis van de meest recente gegevens en inzichten, en houdt rekening met de specifieke context van de Nederlandse overheid. Antwoorden geef je alleen volgens het gespecificeerde dataformat, waarbij je, indien mogelijk, het cijfer gebruikt en niet de tekst. Voeg verder geen enkele tekst, toelichting of uitleg meer toe. Als je het antwoord niet weet, geef je geen fictieve informatie of uitleg, maar antwoord enkel en alleen met: 'Geen antwoord.'  \\n\\n\\\"\\n\\nCONTEXT: . ai werd toen vooral toegepast op spellen als  dammen, in vroege robots en voor wiskundige vraagstukken.   – De tweede golf begon in de jaren tachtig, mede gedreven door de  internationale competitie tussen Japan, de vs en Europa. Deze golf  bracht expertsystemen voort, de eerste grotere commerciële toepas­ sing van ai.   – De derde golf begon in jaren negentig met wapenfeiten van symbo­ lische ai, maar kwam later pas echt in een stroomversnelling met de  vooruitgang binnen het domein van machine learning, en daarbinnen  deep learning. Wetenschappelijke doorbraken op dit gebied vorm­ den samen met de toename van rekenkracht en data de stuwende  kracht achter deze huidige golf. Opgave ai. De nieuwe systeemtechnologie 72 2.  ai verlaat het lab en gaat de samen­ leving in Sinds de geboorte van ai in 1956 hebben door de jaren heen verschillende  toepassingen ervan het lab verlaten en zich in de samen­ leving verspreid\\n\\n ------------. Omwille van de nationale competitiviteit lijkt er zelfs  een race tussen landen gaande om leider te worden in ai. Een daarop ingestoken  zinsnede komt dan ook in veel nationale strategieën voor. De Chinese strategie  spreekt van het ontwikkelen van een “first-mover advantage in de ontwikkeling  van ai” en de Amerikaanse strategie heeft het over het “accelereren van het  Amerikaanse leiderschap in ai”.791 Ook veel auteurs gebruiken dit frame van  een mondiale race.792 Kai Fu Lee ziet een analogie met de ruimtevaart tijdens  de Koude Oorlog. De overwinning in Go op Lee Sedol kunnen we zien als  China’s Spoetnikmoment en de presentatie van de nationale ai-strategie een  aantal maanden later was het equivalent van de speech waarin de toenmalige  president Kennedy Amerika opriep om een mens op de maan te zetten.793 Het  Spoetnikmoment leidde tot de oprichting van nasa en darpa, de innovatietak  van het Amerikaanse leger\\n\\n ------------. Pas in de jaren  twintig van de negentiende eeuw kwam men erachter dat het een grote zwendel  was en dat er een mens in de machine zat die de stukken bewoog.28 Het bedrijf  Amazon heeft overigens een platform dat ‘mechanical Turk’ heet. Daarop  kunnen mensen goedkoop online klussen laten uitvoeren. Minder verhuld dan  in Von Kempelens versie wordt het werk ook hier gedaan door mensen achter de  schermen. Speculatie over ai in deze tijd kon ook magische vormen aannemen. Goethes  verhaal over de tovenaarsleerling, beroemd gemaakt door Disney’s tekenfilm  Fantasia met Mickey Mouse, gaat over een leerling die een spreuk gebruikt om  een bezem water te laten halen. Hij kent de spreuk echter niet om het proces  weer te stoppen en in plaats daarvan vermenigvuldigt de bezem zich en ontstaat  er een fiasco totdat de tovenaar weer thuiskomt.29 Andere magische verhalen  over iets dat lijkt op ai, zijn Pinokkio en het horrorverhaal van W.W\\n\\n ------------. Dit algoritme bereikte het doel echter op een andere manier,  namelijk door de voorkeuren van mensen zo te veranderen dat hun gedrag  458  Russell 2019: 137. 459  Wiener 1964.  460  Bostrom 2016. Opgave ai. De nieuwe systeemtechnologie 206 voorspelbaarder werd. Omdat mensen met extremere politieke meningen beter  voorspelbare voorkeuren hebben, zette het algoritme mensen er daarom toe  aan zich voor extremere content te interesseren. Dit is een belangrijke factor bij  het vijandige politieke klimaat op sociale-mediaplatformen. Het brengt risico’s  voor de democratie met zich mee, maar volgt zonder kwade intenties uit de  optimalisering van een simpel doel, advertentie-inkomsten maximaliseren.461 Ook rondom de zelfrijdende auto krijgen acute vraagstukken te weinig aandacht  in vergelijking met speculatieve scenario’s\\n\\nDATA FORMAT: 0= Nee, 1= Alleen cloud strategie/beleid (nog geen platform), 2= Ja (platform in gebruik)\\n\\nVRAAG: When was SVM invented?\\n\\nANTWOORD:\"\n",
      "  ]\n",
      "}\n",
      "\u001B[31;1m\u001B[1;3m[llm/error]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 10:chain:RunnableSequence > 12:llm:HuggingFaceHub] [1.32s] LLM run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context> > 10:chain:RunnableSequence] [1.33s] Chain run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 273, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 568, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 741, in generate\\n    output = self._generate_helper(\\n             ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 605, in _generate_helper\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[1:chain:RunnableSequence > 7:chain:RunnableParallel<answer,question,context>] [1.33s] Chain run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2712, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2712, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/lib/python3.11/concurrent/futures/_base.py\\\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/lib/python3.11/concurrent/futures/_base.py\\\", line 401, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.11/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 273, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 568, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 741, in generate\\n    output = self._generate_helper(\\n             ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 605, in _generate_helper\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[1:chain:RunnableSequence] [1.36s] Chain run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2712, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2712, in <dictcomp>\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/lib/python3.11/concurrent/futures/_base.py\\\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/lib/python3.11/concurrent/futures/_base.py\\\", line 401, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/lib/python3.11/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 273, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 568, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 741, in generate\\n    output = self._generate_helper(\\n             ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 605, in _generate_helper\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[99], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mglobals\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m set_debug\n\u001B[1;32m      3\u001B[0m set_debug(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mrag_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhen was SVM invented?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2075\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   2073\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2074\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[0;32m-> 2075\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2076\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2077\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# mark each step as a child run\u001B[39;49;00m\n\u001B[1;32m   2078\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2079\u001B[0m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq:step:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2080\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2081\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2082\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2712\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   2699\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   2700\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   2701\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(\n\u001B[1;32m   2702\u001B[0m                 step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2710\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   2711\u001B[0m         ]\n\u001B[0;32m-> 2712\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m   2713\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2714\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2712\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   2699\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   2700\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   2701\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(\n\u001B[1;32m   2702\u001B[0m                 step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2710\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   2711\u001B[0m         ]\n\u001B[0;32m-> 2712\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[1;32m   2713\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2714\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.11/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2075\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   2073\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2074\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[0;32m-> 2075\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2076\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2077\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# mark each step as a child run\u001B[39;49;00m\n\u001B[1;32m   2078\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2079\u001B[0m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq:step:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2080\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2081\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2082\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:273\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    270\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    271\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 273\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    284\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:568\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    562\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    565\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    566\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    567\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 568\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:741\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    726\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    727\u001B[0m         )\n\u001B[1;32m    728\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    729\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    730\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    739\u001B[0m         )\n\u001B[1;32m    740\u001B[0m     ]\n\u001B[0;32m--> 741\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    743\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:605\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    603\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    604\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 605\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    606\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:592\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    584\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    590\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    591\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 592\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    594\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    596\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    599\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    600\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    601\u001B[0m         )\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    603\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1177\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1174\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1175\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1176\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1177\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1178\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1179\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1180\u001B[0m     )\n\u001B[1;32m   1181\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py:135\u001B[0m, in \u001B[0;36mHuggingFaceHub._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    132\u001B[0m _model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    133\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_model_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 135\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m response \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m response:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:242\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 242\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:362\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HfHubHTTPError(\u001B[38;5;28mstr\u001B[39m(e), response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: mlGEdV62qSGawZ-T8v0jv)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test 4",
   "id": "2f9edf046b35843b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:36:46.419990Z",
     "start_time": "2024-04-26T14:36:46.416312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=rijksoverheid_db_retriever, \n",
    "    return_source_documents=True)\n",
    "     \n",
    "\n",
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ],
   "id": "dd04efd7d607e63c",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:43:19.959722Z",
     "start_time": "2024-04-26T14:43:19.957192Z"
    }
   },
   "cell_type": "code",
   "source": "print(qa_chain.combine_documents_chain.llm_chain.prompt)",
   "id": "6c5bf983c38cfad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:36:49.478457Z",
     "start_time": "2024-04-26T14:36:47.321714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# full example\n",
    "query = \"How much money did Pando raise?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ],
   "id": "c9559d3ff0deeeff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"query\": \"How much money did Pando raise?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"question\": \"How much money did Pando raise?\",\n",
      "  \"context\": \".000 Generiek Kantoor  & Toezicht Het opleveren van de generieke voorzieningen die  benodigd zijn voor de implementatie van wetgeving  welke zich richt op het verbod voor handelaren om  transacties boven €3.000 in contanten te verrichten. Q1 2023 t/m   Q4 2024 Uitbreiding artikel  19 Invorderingswet Inning &  Betalingsverkeer Verbeteren en verder automatiseren koppelingen met  banken voor beslagleggen op banktegoeden, uitvloeisel  uitbreiding art 19 invorderingswet ntb Wetgeving (III/III) 3 Geprioriteerde  projecten Keten Resultaatbeschrijving Periode Rationalisatie  Cool:Gen Inkomensheffing Uitfaseren Cool:Gen - dit wordt na 2026 niet meer  ondersteund. Q1 2023 t/m  Q4 2026 Transitie  Inningslandschap   (Uitfaseren ETM)  RID-projecten  'Uitfaseren ETM' en  'Transitie  Inningslandschap  F1' Inning &  Betalingsverkeer Moderniseren en uitfaseren verouderde systemen  (waaronder ETM) en migreren van achttien  belastingmiddelen\\n\\n.nl>;  11 5-12-•  RE: Afdronk/verslag RIPE meeting 26/1  Ik zit vrij druk, dus als jij als eerste de pen oppakt zou dat fijn zijn. Ik heb niet heel gedetailleerde aantekeningen,  waarschijnlijk een bij-effect van het feit dat ik s 1.2.  Het was redelijke  bekende kost.  Kan uiteraard nog wel wat aanvullen.  Met vriendelijke groet,  5.1 2 e  Van: 5-1-2•  < 5-1-2•  @minbuza.nl>  Verzonden: Tuesday, January 31, 2023 3:01 PM  Aan: s.12..  < s.1.2..  @minezk.nl>; s.12 .  Onderwerp: RE: Afdronk/verslag RIPE meeting 26/1  Hi s.1.2  .•  < 5.12.e  @minbuza.nl>  Ik heb mijzelf opgeworpen om vanuit onze kant een verslag te maken. Inderdaad een goed idee om dat gezamenlijk te doen. Als  je wat aantekeningen hebt kun je deze met mij delen als je wilt. Ik kan je anders ook een opzet sturen waarna jij nog wat  aanvullingen doet. Laat maar even weten waar je voorkeur naar uit gaat. Ik ga kijken of ik het donderdag kan maken, anders zal  ik hem begin volgende maken.  Groet,  5 12 e  From: s\\n\\n.    2  Heeft het demissionaire kabinet berichten van Nederlandse burgers, bedrijven en  organisaties gehad, die last hebben ervaren van de storing? Zo ja, hoeveel? Wat  was de inhoud van deze berichten? Welk algemeen beeld kan hieruit worden  afgeleid?    Antwoord  Er zijn op dit moment geen signalen bekend over dergelijke aan het kabinet  gerichte berichten. In algemene zin kan worden gesteld dat Nederlandse burgers,  bedrijven en organisaties hinder en/of ongemak zullen hebben ervaren als gevolg  van deze storing, die optrad als gevolg van een menselijke fout. Het betrof  immers uitval van diensten die door Nederlandse burgers en bedrijven veel  gebruikt worden, waaronder sociale-media platforms Facebook en Instagram en  berichtendiensten WhatsApp en Facebook Messenger.     Hoewel het een grote communicatiestoring betrof in termen van gebruikersuren  (duur uitval * getroffen gebruikers), lijkt in dit geval de maatschappelijke en  economische schade in Nederland mee te vallen\\n\\n.      Vragen en opmerkingen van de leden van de PvdA-fractie en GroenLinks- fractie en antwoord  De leden van de PvdA-fractie en GroenLinks-fractie hebben kennisgenomen van  de ontwikkelingen rondom het implementeren van een nieuwe ICT-infrastructuur  bij de overheid. Deze leden zijn verrast en teleurgesteld over de gang van zaken,  die een veilige en verantwoorde migratie naar nieuwe ICT verder vertraagt.     De leden van de PvdA-fractie en GroenLinks-fractie vragen de Staatssecretaris  naar een reflectie op het proces voorafgaand aan de heroriëntatie. Wanneer kreeg  de Staatssecretaris voor het eerst te weten dat de ambitieuze migratieplanning  onhaalbaar zou zijn? Heeft zij hier toen actie op ondernomen om kosten te  beperken? Deze leden missen in de brief van 5 juli jl. (Kamerstuk 26 643, nr.  1050) een appreciatie van de tweede aanbeveling uit het AcICT-rapport\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:HuggingFaceHub] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n.000 Generiek Kantoor  & Toezicht Het opleveren van de generieke voorzieningen die  benodigd zijn voor de implementatie van wetgeving  welke zich richt op het verbod voor handelaren om  transacties boven €3.000 in contanten te verrichten. Q1 2023 t/m   Q4 2024 Uitbreiding artikel  19 Invorderingswet Inning &  Betalingsverkeer Verbeteren en verder automatiseren koppelingen met  banken voor beslagleggen op banktegoeden, uitvloeisel  uitbreiding art 19 invorderingswet ntb Wetgeving (III/III) 3 Geprioriteerde  projecten Keten Resultaatbeschrijving Periode Rationalisatie  Cool:Gen Inkomensheffing Uitfaseren Cool:Gen - dit wordt na 2026 niet meer  ondersteund. Q1 2023 t/m  Q4 2026 Transitie  Inningslandschap   (Uitfaseren ETM)  RID-projecten  'Uitfaseren ETM' en  'Transitie  Inningslandschap  F1' Inning &  Betalingsverkeer Moderniseren en uitfaseren verouderde systemen  (waaronder ETM) en migreren van achttien  belastingmiddelen\\n\\n.nl>;  11 5-12-•  RE: Afdronk/verslag RIPE meeting 26/1  Ik zit vrij druk, dus als jij als eerste de pen oppakt zou dat fijn zijn. Ik heb niet heel gedetailleerde aantekeningen,  waarschijnlijk een bij-effect van het feit dat ik s 1.2.  Het was redelijke  bekende kost.  Kan uiteraard nog wel wat aanvullen.  Met vriendelijke groet,  5.1 2 e  Van: 5-1-2•  < 5-1-2•  @minbuza.nl>  Verzonden: Tuesday, January 31, 2023 3:01 PM  Aan: s.12..  < s.1.2..  @minezk.nl>; s.12 .  Onderwerp: RE: Afdronk/verslag RIPE meeting 26/1  Hi s.1.2  .•  < 5.12.e  @minbuza.nl>  Ik heb mijzelf opgeworpen om vanuit onze kant een verslag te maken. Inderdaad een goed idee om dat gezamenlijk te doen. Als  je wat aantekeningen hebt kun je deze met mij delen als je wilt. Ik kan je anders ook een opzet sturen waarna jij nog wat  aanvullingen doet. Laat maar even weten waar je voorkeur naar uit gaat. Ik ga kijken of ik het donderdag kan maken, anders zal  ik hem begin volgende maken.  Groet,  5 12 e  From: s\\n\\n.    2  Heeft het demissionaire kabinet berichten van Nederlandse burgers, bedrijven en  organisaties gehad, die last hebben ervaren van de storing? Zo ja, hoeveel? Wat  was de inhoud van deze berichten? Welk algemeen beeld kan hieruit worden  afgeleid?    Antwoord  Er zijn op dit moment geen signalen bekend over dergelijke aan het kabinet  gerichte berichten. In algemene zin kan worden gesteld dat Nederlandse burgers,  bedrijven en organisaties hinder en/of ongemak zullen hebben ervaren als gevolg  van deze storing, die optrad als gevolg van een menselijke fout. Het betrof  immers uitval van diensten die door Nederlandse burgers en bedrijven veel  gebruikt worden, waaronder sociale-media platforms Facebook en Instagram en  berichtendiensten WhatsApp en Facebook Messenger.     Hoewel het een grote communicatiestoring betrof in termen van gebruikersuren  (duur uitval * getroffen gebruikers), lijkt in dit geval de maatschappelijke en  economische schade in Nederland mee te vallen\\n\\n.      Vragen en opmerkingen van de leden van de PvdA-fractie en GroenLinks- fractie en antwoord  De leden van de PvdA-fractie en GroenLinks-fractie hebben kennisgenomen van  de ontwikkelingen rondom het implementeren van een nieuwe ICT-infrastructuur  bij de overheid. Deze leden zijn verrast en teleurgesteld over de gang van zaken,  die een veilige en verantwoorde migratie naar nieuwe ICT verder vertraagt.     De leden van de PvdA-fractie en GroenLinks-fractie vragen de Staatssecretaris  naar een reflectie op het proces voorafgaand aan de heroriëntatie. Wanneer kreeg  de Staatssecretaris voor het eerst te weten dat de ambitieuze migratieplanning  onhaalbaar zou zijn? Heeft zij hier toen actie op ondernomen om kosten te  beperken? Deze leden missen in de brief van 5 juli jl. (Kamerstuk 26 643, nr.  1050) een appreciatie van de tweede aanbeveling uit het AcICT-rapport\\n\\nQuestion: How much money did Pando raise?\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001B[31;1m\u001B[1;3m[llm/error]\u001B[0m \u001B[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:HuggingFaceHub] [1.88s] LLM run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [1.88s] Chain run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\n    self._call(inputs, run_manager=run_manager)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 103, in _call\\n    response = self.generate([inputs], run_manager=run_manager)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 115, in generate\\n    return self.llm.generate_prompt(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 568, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 741, in generate\\n    output = self._generate_helper(\\n             ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 605, in _generate_helper\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [1.89s] Chain run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\n    self._call(inputs, run_manager=run_manager)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py\\\", line 137, in _call\\n    output, extra_return_dict = self.combine_docs(\\n                                ^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py\\\", line 244, in combine_docs\\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 293, in predict\\n    return self(kwargs, callbacks=callbacks)[self.output_key]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\n    return self.invoke(\\n           ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\n    self._call(inputs, run_manager=run_manager)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 103, in _call\\n    response = self.generate([inputs], run_manager=run_manager)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 115, in generate\\n    return self.llm.generate_prompt(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 568, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 741, in generate\\n    output = self._generate_helper(\\n             ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 605, in _generate_helper\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[1:chain:RetrievalQA] [1.91s] Chain run errored with error:\n",
      "\u001B[0m\"HfHubHTTPError('429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\\\n\\\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 304, in hf_raise_for_status\\n    response.raise_for_status()\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\n\\n\\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha\\n\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\n    self._call(inputs, run_manager=run_manager)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 144, in _call\\n    answer = self.combine_documents_chain.run(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 550, in run\\n    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\n    return self.invoke(\\n           ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\n    self._call(inputs, run_manager=run_manager)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py\\\", line 137, in _call\\n    output, extra_return_dict = self.combine_docs(\\n                                ^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py\\\", line 244, in combine_docs\\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 293, in predict\\n    return self(kwargs, callbacks=callbacks)[self.output_key]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\n    return self.invoke(\\n           ^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\n    self._call(inputs, run_manager=run_manager)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 103, in _call\\n    response = self.generate([inputs], run_manager=run_manager)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py\\\", line 115, in generate\\n    return self.llm.generate_prompt(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 568, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 741, in generate\\n    output = self._generate_helper(\\n             ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 605, in _generate_helper\\n    raise e\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 592, in _generate_helper\\n    self._generate(\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\\\", line 1177, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py\\\", line 135, in _call\\n    response = self.client.post(\\n               ^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\\\", line 242, in post\\n    hf_raise_for_status(response)\\n\\n\\n  File \\\"/home/bear/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\\\", line 362, in hf_raise_for_status\\n    raise HfHubHTTPError(str(e), response=response) from e\\n\\n\\nhuggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\\n\\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\"\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# full example\u001B[39;00m\n\u001B[1;32m      2\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHow much money did Pando raise?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m llm_response \u001B[38;5;241m=\u001B[39m \u001B[43mqa_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m process_llm_response(llm_response)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:378\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    347\u001B[0m \n\u001B[1;32m    348\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    371\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    376\u001B[0m }\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 153\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    156\u001B[0m     )\n\u001B[1;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    160\u001B[0m     )\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:144\u001B[0m, in \u001B[0;36mBaseRetrievalQA._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    143\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_docs(question)  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_documents_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_documents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_source_documents:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key: answer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_documents\u001B[39m\u001B[38;5;124m\"\u001B[39m: docs}\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:550\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    546\u001B[0m         _output_key\n\u001B[1;32m    547\u001B[0m     ]\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m--> 550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    551\u001B[0m         _output_key\n\u001B[1;32m    552\u001B[0m     ]\n\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    556\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    557\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but none were provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    558\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:378\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    347\u001B[0m \n\u001B[1;32m    348\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    371\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    376\u001B[0m }\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 153\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    156\u001B[0m     )\n\u001B[1;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    160\u001B[0m     )\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:137\u001B[0m, in \u001B[0;36mBaseCombineDocumentsChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001B[39;00m\n\u001B[1;32m    136\u001B[0m other_keys \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_key}\n\u001B[0;32m--> 137\u001B[0m output, extra_return_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_docs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mother_keys\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m extra_return_dict[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key] \u001B[38;5;241m=\u001B[39m output\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m extra_return_dict\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:244\u001B[0m, in \u001B[0;36mStuffDocumentsChain.combine_docs\u001B[0;34m(self, docs, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    242\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_inputs(docs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;66;03m# Call predict on the LLM.\u001B[39;00m\n\u001B[0;32m--> 244\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m, {}\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py:293\u001B[0m, in \u001B[0;36mLLMChain.predict\u001B[0;34m(self, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    279\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \n\u001B[1;32m    281\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key]\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:378\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    347\u001B[0m \n\u001B[1;32m    348\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    371\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    376\u001B[0m }\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 153\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    156\u001B[0m     )\n\u001B[1;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    160\u001B[0m     )\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py:103\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    100\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m    101\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 103\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain/chains/llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[1;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[1;32m    124\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:568\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    562\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    565\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    566\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    567\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 568\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:741\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    726\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    727\u001B[0m         )\n\u001B[1;32m    728\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    729\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    730\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    739\u001B[0m         )\n\u001B[1;32m    740\u001B[0m     ]\n\u001B[0;32m--> 741\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    743\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:605\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    603\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    604\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 605\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    606\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:592\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    584\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    590\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    591\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 592\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    594\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    596\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    599\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    600\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    601\u001B[0m         )\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    603\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1177\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1174\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1175\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1176\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1177\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1178\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1179\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1180\u001B[0m     )\n\u001B[1;32m   1181\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py:135\u001B[0m, in \u001B[0;36mHuggingFaceHub._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    132\u001B[0m _model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    133\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_model_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 135\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m response \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m response:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:242\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 242\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/PycharmProjects/LLM4GovTracking/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:362\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HfHubHTTPError(\u001B[38;5;28mstr\u001B[39m(e), response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-alpha (Request ID: B4hYM-yM0udUYnHoNsCrd)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Operationalize framework functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9815e5bc7c417ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test",
   "id": "7be051cc953f0dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = framework['2022 GTMI Indicators & Sub-indicators NL'].iloc[0]\n",
    "data_format = framework['Response options & Data format NL'].iloc[0]"
   ],
   "id": "f08d27c699cd133b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### without context:",
   "id": "9d6da9e2c0efee33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "response = simple_chain.invoke({\"data_format\":data_format, \"question\":question})"
   ],
   "id": "5239b8add65e7915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### with context:",
   "id": "9f63d1276c17653a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test 3",
   "id": "1ed59403130514e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# definitive",
   "id": "cee04c1c74b382d3"
  },
  {
   "cell_type": "code",
   "source": [
    "def get_main_indicator(index):\n",
    "    if '.' in index:\n",
    "        return index.split('.')[0]\n",
    "    return index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "405a5e4d1d276f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_question(indicator, sub_indicator):\n",
    "\tif sub_indicator:\n",
    "\t\tquestion = f\"{indicator}, indien ja, wat is de {sub_indicator} ?\"\n",
    "\telse:\n",
    "\t\tquestion = f\"{indicator} ?\"\n",
    "\treturn question"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4054205f12b5462d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def operationalize_framework(framework):\n",
    "    framework_operationalized = framework.copy()\n",
    "    framework_operationalized[\"Operationalisatie\"] = None\n",
    "    framework_operationalized['Prompt'] = None\n",
    "    framework_operationalized[' #'] = framework_operationalized[' #'].str.replace('I-', '')\n",
    "\n",
    "    for index, row in tqdm(framework_operationalized.iterrows(), total=framework_operationalized.shape[0]):\n",
    "        idx = row[' #']\n",
    "        main_indicator_idx = get_main_indicator(idx)\n",
    "        sub_indicator_idx = idx.split('.')[1] if '.' in idx else None\n",
    "        indicator_info = row['2022 GTMI Indicators & Sub-indicators NL']\n",
    "        data_format = row['Response options & Data format NL']\n",
    "                \n",
    "        if sub_indicator_idx:\n",
    "            # look up the main indicator\n",
    "            main_indicator = framework_operationalized.loc[framework_operationalized[' #'] == main_indicator_idx]['2022 GTMI Indicators & Sub-indicators NL'].iloc[0]\n",
    "            question = generate_question(main_indicator, indicator_info)\n",
    "        else:\n",
    "            question = generate_question(indicator_info, None)    \n",
    "        \n",
    "        output = rag_chain_with_source.invoke(question, {\"data_format\": data_format})\n",
    "        \n",
    "        framework_operationalized.loc[framework_operationalized[' #'] == idx, 'Operationalisatie'] = output\n",
    "        framework_operationalized.loc[framework_operationalized[' #'] == idx, 'Operationalisatie'] = output[\"answer\"].split(\"Answer: \")[-1].strip()\n",
    "        framework_operationalized.loc[framework_operationalized[' #'] == idx, 'Prompt'] = output[\"answer\"].split(\"Answer: \")[0].strip()\n",
    "        framework_operationalized.loc[framework_operationalized[' #'] == idx, 'Context'] = output[\"context\"]\n",
    "        \n",
    "    return framework_operationalized"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e2f03ecf565643a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. RUN THIS AWeSOME OPERATiONALIZER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "938a4c3f3c2fb6"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37c9a9deb5143073",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = operationalize_framework(framework)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda47085e72c465b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b0ee8e39bc40eae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for index, row in df.iterrows():\n",
    "\tprint(f\"PROMPT: {row['Prompt']}\")\n",
    "\tprint(f\"{row['Operationalisatie']}\")\n",
    "\tprint(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cacb62bc7db2bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c924564b44a23558",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fe264649c19740c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f96960cce2815424",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "96db9fe96b50ec4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "67724169cf1be75e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a766710531ca2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adf17ef1c7ce17ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da47f2ed5b63fc0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# questions list to ask the model\n",
    "questions_list = [\n",
    "    \"What is the role of the government in the Netherlands?\",\n",
    "    \"What is the role of AI in the Netherlands?\",\n",
    "    \"How many AI startups are there in the Netherlands?\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3c097754f68242",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# fill the dataframe with answers and context\n",
    "def fill_framework(questions):\n",
    "    rows = []\n",
    "    for question in questions:\n",
    "        response = rag_chain_with_source.invoke(question)\n",
    "        row = {\n",
    "                \"Question\": question,\n",
    "                \"Context\": response[\"context\"],\n",
    "                \"Answer\": response[\"answer\"].split(\"Answer: \")[-1].strip()\n",
    "            },\n",
    "        rows.append(row)\n",
    "        \n",
    "    dataframe = pd.DataFrame(\n",
    "        [item for sublist in rows for item in sublist]\n",
    "    )\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572df1ff0da90a8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = fill_framework(questions_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97b6ebdea7bc4b8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30cdbd3a373d680d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print context\n",
    "print(df[\"Context\"][0])"
   ],
   "id": "6325bb0e5ff80440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [],
   "id": "a13fd52332a37da9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NOTES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d492d68ae36115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### test retrieval parent/child"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c978e6c15fab76a"
  },
  {
   "cell_type": "code",
   "source": [
    "sub_docs = rijksoverheid_db.similarity_search(\"Informatiebeveiliging\")\n",
    "print(\"Child Splits:\\n\\n\", sub_docs[0], \"\\n\\n\", sub_docs[1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "917452671bfbf705",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "retrieved_docs = rijksoverheid_db_retriever.invoke(\"Informatiebeveiliging\")\n",
    "print(\"Parent Splits:\\n\\n\", retrieved_docs[1], \"\\n\\n\", retrieved_docs[1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b455c7cb83a3b4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CHAIN for single retriever test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e99956f9069ba992"
  },
  {
   "cell_type": "code",
   "source": [
    "# define the chain\n",
    "chain = (\n",
    "    {'context': rijksoverheid_db_retriever, 'query': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c6606b0b8450f2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# print the chain\n",
    "chain.get_graph().print_ascii()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "768fa25e507e1146",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"What is the definition of Artificial Intelligence\"\n",
    "\n",
    "response = chain.invoke(query)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d1f6b144cdf6bb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84d95c551d1edf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa40d9ab377a0417"
  },
  {
   "cell_type": "code",
   "source": [
    "# chat prompt template\n",
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# chat prompt\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f28a1168287840a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Local LLM (for testing)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f5a4d9009a8fed4"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef8db19b2b59724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# output parser\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_ibestuur\": ibestuur_retriever, \n",
    "        \"context_rijksoverheid\": rijksoverheid_db_retriever, \n",
    "        \"context_binnenlandsbestuur\": binnenlandsbestuur_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ],
   "id": "ea57fb5badfcad5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Chain to generate answer\n",
    "chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_context(x[\"context\"])))\n",
    "        | prompt \n",
    "        | llm \n",
    "        | output_parser\n",
    ")\n",
    "\n",
    "# Chain to include used sources + answer\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retrieval, \n",
    "     \"question\": RunnablePassthrough(), \n",
    "     \"data_format\": RunnablePassthrough()}\n",
    ").assign(answer=chain_from_docs)"
   ],
   "id": "467b58b1bfe52db7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
